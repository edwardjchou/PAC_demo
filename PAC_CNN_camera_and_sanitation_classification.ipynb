{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from keras import applications\n",
    "from keras.preprocessing import image\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import optimizers\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dropout, Flatten, Dense, Input\n",
    "from keras.initializers import glorot_uniform\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from keras.models import load_model\n",
    "\n",
    "\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "from skimage import color\n",
    "from skimage.transform import resize\n",
    "\n",
    "from IPython.display import Audio\n",
    "from IPython.display import display\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25740\n",
      "8580\n",
      "8580\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "print(len(np.load('pac_arrays/train_all.npy')))\n",
    "print(len(np.load('pac_arrays/test_all.npy')))\n",
    "print(len(np.load('pac_arrays/dev_all.npy')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112036\n",
      "112036\n",
      "{'04': 1, '72': 12, '11': 4, '39': 7, '23': 6, '59': 9, '06': 2, '15': 5, '62': 10, '02': 0, '08': 3, '63': 11, '52': 8}\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "with open(\"pac_guide.csv\") as file:\n",
    "    reader = csv.reader(file)\n",
    "    i = 0\n",
    "    image_paths = []\n",
    "    camera_nums = []\n",
    "    labels = []\n",
    "    \n",
    "    for row in reader:\n",
    "        index = row[0]\n",
    "        camera_num = row[1]\n",
    "        label = row[2]\n",
    "        filename = row[3]\n",
    "        image_paths.append(filename)\n",
    "        camera_nums.append(camera_num)\n",
    "        labels.append(label)\n",
    "        #print(index, camera_num, label, filename)\n",
    "print(len(image_paths)) # number of rows: 112036\n",
    "print(len(camera_nums))\n",
    "\n",
    "sub_dir_hash = {}\n",
    "for i, sub_dir in enumerate(np.load('sub_dirs.npy')):\n",
    "    sub_dir_hash[sub_dir] = i\n",
    "\n",
    "print(sub_dir_hash)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112036\n",
      "11994\n",
      "100042\n"
     ]
    }
   ],
   "source": [
    "counter1 = 0\n",
    "counter0 = 0\n",
    "for i in labels:\n",
    "    if i == '1':\n",
    "        counter1 += 1\n",
    "    if i == '0':\n",
    "        counter0 += 1\n",
    "print(len(labels))\n",
    "print(counter1)\n",
    "print(counter0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def depth_map_to_image(depth_map):\n",
    "    img = cv2.normalize(depth_map, depth_map, 0, 1, cv2.NORM_MINMAX)\n",
    "    img = np.array(img * 255, dtype=np.uint8)\n",
    "    orig_img = img\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)\n",
    "    img = cv2.applyColorMap(img, cv2.COLORMAP_OCEAN)\n",
    "    #img = resize(img, 224, 224)\n",
    "\n",
    "    return orig_img, img\n",
    "\n",
    "def image_generator(indices, batch_size):\n",
    "\n",
    "    num_batches = int(len(indices) / batch_size)\n",
    "    \n",
    "    while True:\n",
    "        for batch_i in range(num_batches):\n",
    "            if batch_i == num_batches - 1:\n",
    "                # special case: return as many as possible\n",
    "                start_i = batch_i * batch_size\n",
    "                batch_indices = indices[start_i:]\n",
    "                \n",
    "                X = np.zeros((len(batch_indices), 224, 224, 3))\n",
    "                Y = np.zeros((len(batch_indices), 2)) # Change to one-hot\n",
    "            \n",
    "            else:\n",
    "                start_i = batch_i * batch_size\n",
    "                end_i = start_i + batch_size\n",
    "\n",
    "                batch_indices = indices[start_i:end_i]\n",
    "\n",
    "                X = np.zeros((batch_size, 224, 224, 3))\n",
    "                Y = np.zeros((batch_size, 2)) # Change to one-hot\n",
    "            \n",
    "            for i, index in enumerate(batch_indices):\n",
    "                #img = image.load_img(image_paths[index], target_size=(224, 224))\n",
    "                data = np.load(image_paths[int(index)])\n",
    "                depth_map = data['x'].astype(np.float32)\n",
    "                orig_img, ocean = depth_map_to_image(depth_map)\n",
    "                ocean = resize(ocean, (224, 224), mode='constant')\n",
    "                \n",
    "                X[i, :, :, :] = ocean\n",
    "                # Convert to 1 hot vector\n",
    "                one_hot = np.zeros(2)\n",
    "                #camera_num = camera_nums[int(index)]\n",
    "                #camera_ind = sub_dir_hash[camera_num]\n",
    "                if labels[int(index)] == '1':\n",
    "                    one_hot[1] = 1\n",
    "                elif labels[int(index)] == '0':\n",
    "                    one_hot[0] = 1\n",
    "                    \n",
    "                #one_hot[camera_ind] = 1\n",
    "                Y[i,:] = one_hot\n",
    "            \n",
    "            # use vgg16 preprocessing\n",
    "            X = preprocess_input(X)\n",
    "            \n",
    "            yield (X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "57942016/58889256 [============================>.] - ETA: 0s"
     ]
    }
   ],
   "source": [
    "input_tensor = Input(shape=(224,224,3))\n",
    "model = applications.VGG16(weights='imagenet', include_top=False, input_tensor = input_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# build a classifier model to put on top of the convolutional model\n",
    "x = model.output\n",
    "x = Flatten(input_shape=(model.output_shape[1:]))(x)\n",
    "x = Dropout(0.2)(x)\n",
    "x = Dense(256, activation='relu', kernel_initializer='glorot_uniform')(x)\n",
    "x = Dense(2, activation='softmax', name='output', kernel_initializer='glorot_uniform')(x)\n",
    "\n",
    "# add new classifier model on top of convolutional base\n",
    "new_model = Model(model.input, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for layer in new_model.layers[:19]:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               6422784   \n",
      "_________________________________________________________________\n",
      "output (Dense)               (None, 2)                 514       \n",
      "=================================================================\n",
      "Total params: 21,137,986\n",
      "Trainable params: 6,423,298\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "new_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# RMSprop\n",
    "new_model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=optimizers.RMSprop(lr=0.0001, rho=0.9, epsilon=1e-07, decay=0.0), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/site-packages/ipykernel_launcher.py:27: UserWarning: The semantics of the Keras 2 argument `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Basically steps_per_epoch = samples_per_epoch/batch_size. Similarly `nb_val_samples`->`validation_steps` and `val_samples`->`steps` arguments have changed. Update your method calls accordingly.\n",
      "/usr/local/lib/python3.5/site-packages/ipykernel_launcher.py:27: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<generator..., epochs=30, class_weight={0: 1.0, 1..., steps_per_epoch=403, validation_data=<generator..., validation_steps=135)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "403/403 [==============================] - 570s - loss: 1.5656 - acc: 0.5040 - val_loss: 1.1046 - val_acc: 0.1484\n",
      "Epoch 2/30\n",
      "403/403 [==============================] - 536s - loss: 1.3110 - acc: 0.5495 - val_loss: 0.6114 - val_acc: 0.6933\n",
      "Epoch 3/30\n",
      "403/403 [==============================] - 536s - loss: 1.2668 - acc: 0.5731 - val_loss: 1.8618 - val_acc: 0.1482\n",
      "Epoch 4/30\n",
      "403/403 [==============================] - 535s - loss: 1.2298 - acc: 0.5935 - val_loss: 0.9156 - val_acc: 0.3047\n",
      "Epoch 5/30\n",
      "403/403 [==============================] - 535s - loss: 1.2069 - acc: 0.6001 - val_loss: 1.0624 - val_acc: 0.2587\n",
      "Epoch 6/30\n",
      "403/403 [==============================] - 535s - loss: 1.1880 - acc: 0.6105 - val_loss: 1.0760 - val_acc: 0.2698\n",
      "Epoch 7/30\n",
      "403/403 [==============================] - 537s - loss: 1.1827 - acc: 0.6156 - val_loss: 0.5438 - val_acc: 0.7420\n",
      "Epoch 8/30\n",
      "403/403 [==============================] - 536s - loss: 1.1650 - acc: 0.6194 - val_loss: 1.6740 - val_acc: 0.1526\n",
      "Epoch 9/30\n",
      "403/403 [==============================] - 535s - loss: 1.1562 - acc: 0.6293 - val_loss: 0.7679 - val_acc: 0.5155\n",
      "Epoch 10/30\n",
      "403/403 [==============================] - 537s - loss: 1.1460 - acc: 0.6347 - val_loss: 0.6668 - val_acc: 0.6541\n",
      "Epoch 11/30\n",
      "403/403 [==============================] - 535s - loss: 1.1370 - acc: 0.6378 - val_loss: 0.8566 - val_acc: 0.4252\n",
      "Epoch 12/30\n",
      "403/403 [==============================] - 535s - loss: 1.1295 - acc: 0.6367 - val_loss: 0.4478 - val_acc: 0.8001\n",
      "Epoch 13/30\n",
      "403/403 [==============================] - 535s - loss: 1.1261 - acc: 0.6402 - val_loss: 0.7117 - val_acc: 0.5941\n",
      "Epoch 14/30\n",
      "403/403 [==============================] - 535s - loss: 1.1130 - acc: 0.6506 - val_loss: 0.6020 - val_acc: 0.7025\n",
      "Epoch 15/30\n",
      "403/403 [==============================] - 535s - loss: 1.1116 - acc: 0.6477 - val_loss: 1.3475 - val_acc: 0.2071\n",
      "Epoch 16/30\n",
      "403/403 [==============================] - 535s - loss: 1.1049 - acc: 0.6524 - val_loss: 0.5303 - val_acc: 0.7495\n",
      "Epoch 17/30\n",
      "403/403 [==============================] - 535s - loss: 1.0968 - acc: 0.6532 - val_loss: 0.8074 - val_acc: 0.4817\n",
      "Epoch 18/30\n",
      "403/403 [==============================] - 534s - loss: 1.0990 - acc: 0.6528 - val_loss: 1.2915 - val_acc: 0.2549\n",
      "Epoch 19/30\n",
      "403/403 [==============================] - 534s - loss: 1.0904 - acc: 0.6575 - val_loss: 0.7657 - val_acc: 0.5267\n",
      "Epoch 20/30\n",
      "403/403 [==============================] - 534s - loss: 1.0847 - acc: 0.6596 - val_loss: 0.9725 - val_acc: 0.3719\n",
      "Epoch 21/30\n",
      "403/403 [==============================] - 534s - loss: 1.0809 - acc: 0.6591 - val_loss: 0.6230 - val_acc: 0.6934\n",
      "Epoch 22/30\n",
      "403/403 [==============================] - 534s - loss: 1.0727 - acc: 0.6612 - val_loss: 1.1622 - val_acc: 0.2815\n",
      "Epoch 23/30\n",
      "403/403 [==============================] - 534s - loss: 1.0736 - acc: 0.6615 - val_loss: 0.7377 - val_acc: 0.5658\n",
      "Epoch 24/30\n",
      "403/403 [==============================] - 535s - loss: 1.0708 - acc: 0.6664 - val_loss: 0.8176 - val_acc: 0.4817\n",
      "Epoch 25/30\n",
      "403/403 [==============================] - 537s - loss: 1.0769 - acc: 0.6614 - val_loss: 0.5211 - val_acc: 0.7547\n",
      "Epoch 26/30\n",
      "403/403 [==============================] - 536s - loss: 1.0693 - acc: 0.6677 - val_loss: 0.6844 - val_acc: 0.6448\n",
      "Epoch 27/30\n",
      "403/403 [==============================] - 535s - loss: 1.0628 - acc: 0.6661 - val_loss: 0.6440 - val_acc: 0.6735\n",
      "Epoch 28/30\n",
      "403/403 [==============================] - 535s - loss: 1.0620 - acc: 0.6697 - val_loss: 0.3847 - val_acc: 0.8314\n",
      "Epoch 29/30\n",
      "403/403 [==============================] - 535s - loss: 1.0580 - acc: 0.6688 - val_loss: 0.4396 - val_acc: 0.7999\n",
      "Epoch 30/30\n",
      "403/403 [==============================] - 534s - loss: 1.0567 - acc: 0.6725 - val_loss: 1.4539 - val_acc: 0.2104\n"
     ]
    }
   ],
   "source": [
    "minibatch_size = 64\n",
    "\n",
    "train_indices = np.load('pac_arrays/train_all.npy')\n",
    "test_indices = np.load('pac_arrays/test_all.npy')\n",
    "\n",
    "np.random.shuffle(train_indices)\n",
    "np.random.shuffle(test_indices)\n",
    "\n",
    "image_generator(train_indices, minibatch_size)\n",
    "\n",
    "epochs = 30\n",
    "minibatch_size = 64\n",
    "\n",
    "train_steps = math.ceil(len(train_indices) / minibatch_size)\n",
    "test_steps = math.ceil(len(test_indices) / minibatch_size)\n",
    "\n",
    "class_weight = {0 : 1.,\n",
    "    1: 7.}\n",
    "\n",
    "# fine-tune the model\n",
    "history = new_model.fit_generator(\n",
    "    image_generator(train_indices, minibatch_size),\n",
    "    steps_per_epoch=train_steps,\n",
    "    epochs=epochs,\n",
    "    class_weight = class_weight,\n",
    "    validation_data=image_generator(test_indices, minibatch_size),\n",
    "    nb_val_samples=test_steps)\n",
    "\n",
    "new_model.save('pca_camera_and_sanitation_classification.hdf5')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                <audio controls=\"controls\" autoplay=\"autoplay\">\n",
       "                    <source src=\"http://www.pacdv.com/sounds/interface_sound_effects/sound82.wav\" type=\"audio/x-wav\" />\n",
       "                    Your browser does not support the audio element.\n",
       "                </audio>\n",
       "              "
      ],
      "text/plain": [
       "<IPython.lib.display.Audio object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                <audio controls=\"controls\" autoplay=\"autoplay\">\n",
       "                    <source src=\"http://www.pacdv.com/sounds/interface_sound_effects/sound82.wav\" type=\"audio/x-wav\" />\n",
       "                    Your browser does not support the audio element.\n",
       "                </audio>\n",
       "              "
      ],
      "text/plain": [
       "<IPython.lib.display.Audio object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                <audio controls=\"controls\" autoplay=\"autoplay\">\n",
       "                    <source src=\"http://www.pacdv.com/sounds/interface_sound_effects/sound82.wav\" type=\"audio/x-wav\" />\n",
       "                    Your browser does not support the audio element.\n",
       "                </audio>\n",
       "              "
      ],
      "text/plain": [
       "<IPython.lib.display.Audio object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                <audio controls=\"controls\" autoplay=\"autoplay\">\n",
       "                    <source src=\"http://www.pacdv.com/sounds/interface_sound_effects/sound82.wav\" type=\"audio/x-wav\" />\n",
       "                    Your browser does not support the audio element.\n",
       "                </audio>\n",
       "              "
      ],
      "text/plain": [
       "<IPython.lib.display.Audio object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                <audio controls=\"controls\" autoplay=\"autoplay\">\n",
       "                    <source src=\"http://www.pacdv.com/sounds/interface_sound_effects/sound82.wav\" type=\"audio/x-wav\" />\n",
       "                    Your browser does not support the audio element.\n",
       "                </audio>\n",
       "              "
      ],
      "text/plain": [
       "<IPython.lib.display.Audio object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "time.sleep(2)\n",
    "sound_file = 'http://www.pacdv.com/sounds/interface_sound_effects/sound82.wav'\n",
    "display(Audio(url=sound_file, autoplay=True))\n",
    "time.sleep(2)\n",
    "display(Audio(url=sound_file, autoplay=True))\n",
    "\n",
    "for i in range(0, 3):\n",
    "    time.sleep(2)\n",
    "    display(Audio(url=sound_file, autoplay=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = load_model('pca_camera_and_sanitation_classification.hdf5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=optimizers.RMSprop(lr=0.0001, rho=0.9, epsilon=1e-07, decay=0.0), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 5\n"
     ]
    }
   ],
   "source": [
    "minibatch_size = 64\n",
    "\n",
    "train_indices = np.load('pac_arrays/train_all.npy')\n",
    "test_indices = np.load('pac_arrays/test_all.npy')\n",
    "\n",
    "image_generator(train_indices, minibatch_size)\n",
    "\n",
    "epochs = 1\n",
    "minibatch_size = 64\n",
    "\n",
    "train_steps = math.ceil(len(train_indices) / minibatch_size)\n",
    "test_steps = math.ceil(len(test_indices) / minibatch_size)\n",
    "\n",
    "num0 = 0\n",
    "num1 = 0\n",
    "\n",
    "for i in train_indices:\n",
    "    i = int(i)\n",
    "    if labels[i] == '0':\n",
    "        num0 += 1\n",
    "    elif labels[i] == '1':\n",
    "        num1 += 1\n",
    "train_ratio = int(num0 / num1)\n",
    "\n",
    "num0 = 0\n",
    "num1 = 0\n",
    "for i in test_indices:\n",
    "    i = int(i)\n",
    "    if labels[i] == '0':\n",
    "        num0 += 1\n",
    "    elif labels[i] == '1':\n",
    "        num1 += 1\n",
    "test_ratio = int(num0 / num1)\n",
    "\n",
    "\n",
    "print(train_ratio, test_ratio)\n",
    "\n",
    "train_weight = {0 : 1.,\n",
    "1: train_ratio}\n",
    "\n",
    "test_weight = {0 : 1.,\n",
    "1: test_ratio}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "403/403 [==============================] - 384s - loss: 0.8989 - acc: 0.7342   \n"
     ]
    }
   ],
   "source": [
    "history = new_model.fit_generator(\n",
    "    image_generator(train_indices, minibatch_size),\n",
    "    steps_per_epoch=train_steps,\n",
    "    epochs=epochs,\n",
    "    class_weight = train_weight\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "135/135 [==============================] - 137s - loss: 0.8921 - acc: 0.7334   \n"
     ]
    }
   ],
   "source": [
    "history = new_model.fit_generator(\n",
    "    image_generator(test_indices, minibatch_size),\n",
    "    steps_per_epoch=test_steps,\n",
    "    epochs=epochs,\n",
    "    class_weight = test_weight\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
