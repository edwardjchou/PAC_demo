{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import yaml\n",
    "\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import csv\n",
    "\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from matplotlib.image import imread\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import csv\n",
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from keras import applications\n",
    "from keras.preprocessing import image\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import optimizers\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dropout, Flatten, Dense, Input\n",
    "from keras.initializers import glorot_uniform\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "\n",
    "import cv2\n",
    "from skimage import color\n",
    "from skimage.transform import resize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "58040320/58889256 [============================>.] - ETA: 0s112036\n",
      "112036\n",
      "{'52': 8, '39': 7, '06': 2, '63': 11, '04': 1, '62': 10, '72': 12, '02': 0, '23': 6, '59': 9, '08': 3, '15': 5, '11': 4}\n"
     ]
    }
   ],
   "source": [
    "model = VGG16(weights='imagenet', include_top=False)\n",
    "\n",
    "import csv\n",
    "with open(\"pac_guide.csv\") as file:\n",
    "    reader = csv.reader(file)\n",
    "    i = 0\n",
    "    image_paths = []\n",
    "    camera_nums = []\n",
    "    labels = []\n",
    "    \n",
    "    for row in reader:\n",
    "        index = row[0]\n",
    "        camera_num = row[1]\n",
    "        label = row[2]\n",
    "        filename = row[3]\n",
    "        image_paths.append(filename)\n",
    "        camera_nums.append(camera_num)\n",
    "        labels.append(label)\n",
    "        #print(index, camera_num, label, filename)\n",
    "print(len(image_paths)) # number of rows: 112036\n",
    "print(len(camera_nums))\n",
    "\n",
    "sub_dir_hash = {}\n",
    "for i, sub_dir in enumerate(np.load('sub_dirs.npy')):\n",
    "    sub_dir_hash[sub_dir] = i\n",
    "\n",
    "print(sub_dir_hash)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_num = '15'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1980\n",
      "660\n"
     ]
    }
   ],
   "source": [
    "train_indices = np.load('pac_arrays/train_' + str(model_num) + '.npy')\n",
    "test_indices = np.load('pac_arrays/test_' + str(model_num) + '.npy')\n",
    "print(len(train_indices))\n",
    "print(len(test_indices))\n",
    "num_images = len(train_indices) + len(test_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def depth_map_to_image(depth_map):\n",
    "    img = cv2.normalize(depth_map, depth_map, 0, 1, cv2.NORM_MINMAX)\n",
    "    img = np.array(img * 255, dtype=np.uint8)\n",
    "    orig_img = img\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)\n",
    "    img = cv2.applyColorMap(img, cv2.COLORMAP_OCEAN)\n",
    "    #img = resize(img, 224, 224)\n",
    "\n",
    "    return orig_img, img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2640, 25088)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "cnn_feats = np.zeros((num_images, 512 * 7 * 7))\n",
    "\n",
    "counter = 0\n",
    "\n",
    "for index in train_indices:\n",
    "\n",
    "    data = np.load(image_paths[int(index)])\n",
    "    depth_map = data['x'].astype(np.float32)\n",
    "    orig_img, ocean = depth_map_to_image(depth_map)\n",
    "    ocean = resize(ocean, (224, 224), mode='constant')\n",
    "    x = image.img_to_array(ocean)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    x = preprocess_input(x)\n",
    "    features = model.predict(x)\n",
    "\n",
    "    # Reshape features into 2D tensor\n",
    "    reshaped_features = features.reshape(1, 512*7*7)\n",
    "    cnn_feats[counter, :] = reshaped_features\n",
    "\n",
    "    counter+=1\n",
    "    \n",
    "for index in test_indices:\n",
    "\n",
    "    data = np.load(image_paths[int(index)])\n",
    "    depth_map = data['x'].astype(np.float32)\n",
    "    orig_img, ocean = depth_map_to_image(depth_map)\n",
    "    ocean = resize(ocean, (224, 224), mode='constant')\n",
    "    x = image.img_to_array(ocean)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    x = preprocess_input(x)\n",
    "    features = model.predict(x)\n",
    "\n",
    "    # Reshape features into 2D tensor\n",
    "    reshaped_features = features.reshape(1, 512*7*7)\n",
    "    cnn_feats[counter, :] = reshaped_features\n",
    "\n",
    "    counter+=1\n",
    "\n",
    "print(cnn_feats.shape)\n",
    "\n",
    "# reduce dimensionality using principal components analysis (PCA)\n",
    "\n",
    "pca = PCA(n_components=256)\n",
    "pca.fit(cnn_feats)\n",
    "cnn_feats_compressed = pca.transform(cnn_feats)\n",
    "\n",
    "np.save('PAC_cnn_pca_components_' + str(model_num), pca.components_)\n",
    "np.save('PAC_cnn_pca_features_' + str(model_num), cnn_feats_compressed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
